# -*- coding: utf-8 -*-
"""YoutubeSentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RRHnYemZpKr1f30J6Z8m7oKw9O824jEE
"""

# Import libraries
import requests
import json
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Initialize sentiment analyzer
nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()

# Define YouTube API key and video ID
api_key = "AIzaSyDXTqebWShuOtyT29ReShkTl5OhQ5pcjAk"
video_id = "C2EvpdSxOQg"

# Define function to get comments from YouTube
def get_comments(api_key, video_id):
    # Initialize empty list to store comments
    comments = []
    # Define base URL for commentThreads API
    base_url = "https://www.googleapis.com/youtube/v3/commentThreads"
    # Define parameters for API request
    params = {
        "part": "id,snippet,replies",
        "videoId": video_id,
        "key": api_key,
        "maxResults": 100 # Change this as needed
    }
    # Loop through pages of results until end
    while True:
        # Make API request and get response as JSON
        response = requests.get(base_url, params=params).json()
        # Loop through items in response and append comment texts to list
        for item in response["items"]:
            comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
            comments.append(comment)
            # Check if comment has replies and append them to list
            if "replies" in item:
                for reply in item["replies"]["comments"]:
                    comment = reply["snippet"]["textDisplay"]
                    comments.append(comment)
        # Check if response has nextPageToken and update params accordingly
        if "nextPageToken" in response:
            params["pageToken"] = response["nextPageToken"]
        else:
            break
    # Return the list of comments
    return comments

# Define function to analyze sentiment of text
def analyze_sentiment(text):
    # Get polarity scores from sentiment analyzer
    scores = sia.polarity_scores(text)
    # Return the compound score as the sentiment
    return scores["compound"]

# Get comments from YouTube using video ID and API key
comments = get_comments(api_key, video_id)

# Loop through comments and print their sentiment scores
for comment in comments:
    sentiment = analyze_sentiment(comment)
    print(f"Comment: {comment}")
    print(f"Sentiment: {sentiment}")
    print()

"""The first part of the code imports the necessary libraries for making HTTP requests, parsing JSON data, and performing natural language processing tasks.
The second part of the code initializes a sentiment analyzer object from the NLTK library. This object will be used to calculate the sentiment scores of the comments. It also downloads a lexicon file that contains a list of words and their polarity values.
The third part of the code defines two variables: api_key and video_id. These variables store the YouTube API key and the video ID that will be used to get the comments. You need to replace these values with your own API key and video ID.
The fourth part of the code defines a function called get_comments that takes two arguments: api_key and video_id. This function will return a list of comments from the specified video using the YouTube Data API.
The function first initializes an empty list called comments that will store the comment texts.
The function then defines a base URL for the commentThreads API endpoint and a dictionary of parameters for the API request. The parameters include the part, videoId, key, and maxResults values. The maxResults value determines how many comments will be returned per page. You can change this value as needed.
The function then enters a while loop that will run until there are no more pages of comments left. Inside the loop, the function makes an HTTP GET request to the base URL with the parameters and gets the response as a JSON object. The function then loops through the items in the response and appends the text of each comment to the comments list. If a comment has replies, it also appends the text of each reply to the list.
The function then checks if the response has a nextPageToken key, which indicates that there are more pages of comments available. If so, it updates the pageToken value in the parameters dictionary with the value of nextPageToken. This way, the next request will get the next page of comments. If there is no nextPageToken key in the response, it means that there are no more comments left and the loop ends.
The function finally returns the comments list as the output.

The fifth part of the code defines another function called analyze_sentiment that takes one argument: text. This function will return a sentiment score for the given text using the sentiment analyzer object from NLTK.
The function first calls the polarity_scores method of the sentiment analyzer object and passes the text as an argument. This method returns a dictionary of scores for different aspects of sentiment: negative, neutral, positive, and compound. The compound score is a normalized value that represents the overall sentiment of the text.
The function then returns the compound score as the output. This score is a number between -1 and 1, where -1 means very negative, 0 means neutral, and 1 means very positive.
The sixth part of the code calls the get_comments function with the api_key and video_id variables as arguments and assigns the output to a variable called comments. This variable will store a list of comments from the specified video.
The seventh part of the code loops through the comments list and prints each comment along with its sentiment score. To get the sentiment score, it calls the analyze_sentiment function with each comment as an argument and assigns the output to a variable called sentiment. It then uses formatted strings to print the comment and the sentiment score in a readable way.

This code uses the SentimentIntensityAnalyzer class from the NLTK library to decide which comment is positive or negative. This class is based on a rule-based algorithm called VADER (Valence Aware Dictionary and sEntiment Reasoner), which assigns polarity values to words based on their semantic orientation and intensity. For example, the word “love” has a positive polarity of 0.6369, while the word “hate” has a negative polarity of -0.6369.

The algorithm also takes into account modifiers, such as negations, intensifiers, and diminishers, that can change the polarity of a word or a phrase. For example, the phrase “not love” has a negative polarity of -0.3875, while the phrase “very love” has a positive polarity of 0.7925.

The algorithm then computes the sentiment score of a text by summing up the polarity values of all the words and phrases in the text and normalizing them to a value between -1 and 1. The sentiment score represents the overall sentiment of the text, where -1 means very negative, 0 means neutral, and 1 means very positive.

The reliability of this algorithm depends on the type and domain of the text that you want to analyze. According to a paper by the creators of VADER1, the algorithm has a high accuracy for general social media texts, especially those that express opinions, emotions, or sentiments. However, it may not perform well for texts that are very formal, factual, or domain-specific, such as news articles, academic papers, or medical reports.

The paper also reports that VADER outperforms other sentiment analysis algorithms on social media texts in terms of precision, recall, and F1-score. However, these results may vary depending on the data set and the evaluation method used. Therefore, it is advisable to test the algorithm on your own data and compare it with other algorithms to find the best one for your needs.
"""